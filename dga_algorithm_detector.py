#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
---------> DOMAIN GENERATION ALGORITHM DETECTOR <--------
Machine learning system to detect domain names which are likely generated by malware as randomized rendezvous points using
a supervised deep learning network. The Domain name, Origin and Target are encoded to categories before being fitted to numerical
values. To preserve the encoding scheme between the training and test set, the training set is saved to be used in category encoding
on new unseen testing set prior to inference.

Created on Wed Mar 27 10:26:19 2019
@author: Andrew JM Kiruluta
"""

import numpy as np
import pandas as pd
import pickle
import argparse
import matplotlib
from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.layers import Dropout
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.layers.normalization import BatchNormalization
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from keras.models import model_from_json
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# fix random seed for reproducibility
seed = 7
np.random.seed(seed)

# clean domain data, correcting for target typos, purging sub-domains and removing
# target nan labels
def _cleanup_data(X):
   X['target'] = X['target'].astype(str)
   X['target'] = X['target'].str.lower()    # make all class label lower case
   
   # purge subdomains from data set
   X['domain'] = ['nan' if item.count('.') > 1 else item for item in X.domain]
   X = X[X.domain != 'nan']
   
   # use NLP to match all targets to either 'legit' or 'dga' to tackle close spellings
   from fuzzywuzzy import fuzz
   X.target = ['legit' if fuzz.ratio(item,'legit') > 50 else item for item in X.target]
   X.target = ['dga'   if fuzz.ratio(item,'dga') > 50 else item  for item in X.target]

   # drop rows with NaNs in target column
   X = X[X.target != 'nan']

   # creating a dict file
   logic = {'legit': 0,'dga': 1}

   # traversing through dataframe values where key matches
   X['target'] = [logic[item] for item in X.target]
   return X 
    
'''
A short, pythonic solution to balance a pandas DataFrame either by subsampling
(uspl=True) or oversampling (uspl=False), balanced by a specified column in that
dataframe that has two or more values.

For uspl=True, this code will take a random sample without replacement of size
 equal to the smallest stratum from all strata. For uspl=False, this code will
 take a random sample with replacement of size equal to the largest stratum
 from all strata.
'''


def _balanced_spl_by(df, col, uspl):
    datas_l = [df[df[col] == l].copy() for l in list(set(df[col].values))]
    lsz = [f.shape[0] for f in datas_l]
    X = pd.concat([f.sample(n=(min(lsz) if uspl else max(lsz)), replace=(not uspl)).copy() \
                   for f in datas_l], axis=0).sample(frac=1)

    uniq_levels = np.unique(X[col])
    uniq_counts = {level: sum(X[col] == level) for level in uniq_levels}
    return X, uniq_counts

def _categorical_preprocess(train, valid, col_names):
    # categorical encoding for DNN:
    frames = [train, valid]
    result = pd.concat(frames)
    df_cat = pd.DataFrame(result, columns=col_names)

    # convert 'domain name' and 'origin' into categories
    df_cat['domain'] = df_cat.domain.astype(str)
    df_cat['domain'] = df_cat.domain.astype('category')
    df_cat['origin'] = df_cat.origin.astype(str)
    df_cat['origin'] = df_cat.origin.astype('category')

    # final feature matrix for train/test
    encoder = LabelEncoder()
    df_cat = df_cat.apply(encoder.fit_transform)

    # scale the data
    df_cat[['domain','origin']] = StandardScaler().fit_transform(df_cat[['domain','origin']])
   
    #df_cat = pd.DataFrame(df_cat, columns=col_names)
    # split train/test and make sure training data is saved for later inference...
    X_train, X_test = df_cat.head(train.shape[0]), df_cat.tail(len(result) - train.shape[0])
    return X_train, X_test


# define neuron network base model
def _baseline_model(no_features):
    model = Sequential()
    model.add(Dense(256, input_dim=no_features, kernel_initializer='normal'))
    model.add(Activation('relu'))
    #model.add(BatchNormalization())
    model.add(Dropout(0.2))
    model.add(Dense(128, kernel_initializer='normal'))
    model.add(Activation('relu'))
    #model.add(BatchNormalization())
    model.add(Dense(64, kernel_initializer='normal'))
    model.add(Activation('relu'))
    #model.add(BatchNormalization())
    model.add(Dropout(0.2))
    model.add(Dense(2, kernel_initializer='normal', activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model


def _train_DNN_Model(X):
    response = 'target'
    X1, counts = _balanced_spl_by(X, response, uspl=True)
    print('new class distribution: %s' % counts)

    y = X[response]  # target
    y = to_categorical(y)  # categorical [legit, dga]
    df = X.drop([response], axis=1)  # feature dataframe

    # train/test split and then DNN model...
    X_train, X_test_orig, y_train, y_test = train_test_split(df, y, test_size=0.1, random_state=seed)
    
    
    # save training set for categorization of new data classes for inference
    # need original model training data for each corresponding inference with
    # that model
    train_file = "X_train" +  ".pkl"
    fileObject = open(train_file, 'wb')
    pickle.dump([X_train], fileObject)
    fileObject.close()

    X_train, X_test = _categorical_preprocess(X_train, X_test_orig, df.columns)

    # build the model
    no_features = X_train.shape[1]
    estimator = _baseline_model(no_features)
    # list all data in history
    print(estimator.summary())

    # checkpoint and save the best model weights in hd5 whenever the validation accuracy improves
    model_weights_path = "weights.best" + ".hdf5"
    checkpoint_weights = ModelCheckpoint(model_weights_path, monitor='val_acc', verbose=1, save_best_only=True,
                                         mode='max')

    # early stopping
    checkpoint_stop = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')
    callbacks_list = [checkpoint_weights, checkpoint_stop]

    # now fit over the entire dataset over the selected model and pickle the weights
    history = estimator.fit(X_train, y_train, validation_split=0.2, shuffle=True, epochs=300, batch_size=128,
                            callbacks=callbacks_list)

    # ---serialize model to JSON & weights to hd5 for later prediction ---#
    model_json = estimator.to_json()
    with open("model.json", "w") as json_file:
        json_file.write(model_json)
        json_file.close()
    return estimator, X_train, X_test, y_test, y_train, history

class _predict_Keras_Model:
    def __init__(self, X):
        self.X = X

    def load_model_from_disk(self):
        # ---> generation of prediction scores for the Multi Class ROC curves<---
        # load json and create model from best validation score
        #json_file = open(os.environ['HOME'] + '/model.json', 'r')
        json_file = open("model.json", "r")
        loaded_model_json = json_file.read()
        json_file.close()
        loaded_model = model_from_json(loaded_model_json)
        # load weights into new model
        model_weights =  "weights.best.hdf5"
        loaded_model.load_weights(model_weights)
        # evaluate loaded model on test data
        loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
        return loaded_model

    # prediction module with Keras Model
    def _predict_DNN_Model(self):
        loaded_model = self.load_model_from_disk()
        pred = loaded_model.predict(self.X)
        return pred

# ROC curve code adapted from scikit-learn page
def _ROC_Curve(n_classes, y_test, y_score):
    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Compute micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

    # plt.figure()
    lw = 2
    plt.plot(fpr[1], tpr[1], color='darkorange',
             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1])
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristics (ROC)')
    plt.legend(loc="lower right")
    plt.show()
    plt.savefig("ROC.png")
    matplotlib.pyplot.show()

def _acc_loss(history):
    plt.figure()
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'val'], loc='upper left')
    plt.show()
    plt.savefig("loss.png")
    matplotlib.pyplot.show()

# argument parser
def parse_cmd_line():
    p = argparse.ArgumentParser(description='Domain Algorithm Detector Algorithm',
                formatter_class = argparse.ArgumentDefaultsHelpFormatter,
                epilog="python dga_algorithm_detector.py -f test_file.txt " )
    p.set_defaults(verbose = 0, quiet=False)
    p.add_argument('-t', '--train', help='train model', default='no')
    p.add_argument('-x', '--test', help='test model', default='yes')
    # test file should be in the same format delimited by 'ctrl-A' separated csv minus the class label
    p.add_argument('-f','--file', help='pass in test file of unseen data for prediction',type=str, required=False)
    args = p.parse_args()
    return args

if __name__ == '__main__':
    args = parse_cmd_line()

    if args.train == 'yes':
       data = pd.read_csv('dga-dataset.txt', sep='\x01', names=['domain','origin','target'])
       # pre-process data prior to training
       clean_data = _cleanup_data(data)  

       model, X_train, X_test, y_test, y_train, history = _train_DNN_Model(clean_data)

       # Keras Prediction Module & ROC Curves
       keras_predict = _predict_Keras_Model(X_test)
       y_pred = keras_predict._predict_DNN_Model()
       print('Generating ROC curve')
       _ROC_Curve(2, y_test, y_pred)

       # ---> Accuracy & Loss Curves
       _acc_loss(history)

    if args.test == 'yes':
       # Keras Prediction Module Entry Point - input file should have no class labels
       Xtest = pd.read_csv(args.file, sep='\x01', names=['domain','origin'])

       # preprocess data into categories and fit to numbers
       train_file = "X_train.pkl"
       fileObject = open(train_file, 'rb')
       Xtrain = pickle.load(fileObject) # for consistent encoder.fit train & test datasets
       X_train, X_test = _categorical_preprocess(Xtrain, Xtest, Xtest.columns)
       keras_predict = _predict_Keras_Model(X_test)
       y_pred = keras_predict._predict_DNN_Model()
       classes = np.argmax(y_pred, axis=1)
       class_label = ['legit' if item == 0 else 'dga' for item in classes]
       print("predictions => {'legit': 0,'dga': 1}: %s" % class_label)
       train_file = "predictions" +  ".pkl"
       fileObject = open(class_label, 'wb')
       pickle.dump([X_train], fileObject)
       fileObject.close()
